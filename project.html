<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.0.0/animate.min.css"
    />
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700;800&family=Mukta&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <!-- CSS -->
    <link rel="stylesheet" href="https://unpkg.com/flickity@2/dist/flickity.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@500&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@500&family=Raleway&display=swap" rel="stylesheet">
    <link href="img/logo1.png" rel="shortcut icon" type="image/x-icon">
    <link href="img/logo1.png" rel="apple-touch-icon">
    <link rel="stylesheet" href="project.css">
    <script src="https://kit.fontawesome.com/4e16efa13b.js" crossorigin="anonymous"></script>
    <title>ROBOISM</title>
</head>
<body>

  <img style="height: 100vh; width: 100vw; background-repeat: no-repeat; background-size: cover; position: fixed; background-position: center; z-index: -1; display: flex;" src="img/background/7.jpg">
  <!--
      https://cutewallpaper.org/21/cool-robot-wallpapers/Dark-Robot-Art-Wallpaper-15496-Wallpaper-Cool-Wallpaper-.jpg
      https://wallpaperfx.com/uploads/wallpapers/2013/05/12/12634/preview_huge-robot-monster.jpeg
      
  -->


    
   <!--===========================================================-->


  <!--PROJECTS-->
  <!--==================================================================================-->


  <div class="container">
    <div id="topone">
        <div class="jumbotron" style="background: rgba(0,0,0,0.8); color: #dfdfdf; padding: 1%; border: 1px solid #d3d3d3; ">
            <h1 style="text-align: center; text-shadow: 2px 2px 6px #f0f0f0;  margin-top: 1%; font-family: 'Montserrat', sans-serif;">Projects</h1>
        </div>
    </div>
  </div>


    <div class="container" id="projectpagemain">
      
        <div class="row">



          <div class="col-md-5" >
            <p data-modal-target="modal2" >
              <img style="width: 100%; " src="img/projects/2.png">
              <div data-modal-target="modal2" class="card-block">
                <h2 id="protit">Flying machine</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Harshit Jain</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Srajan Gupta</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Kaushik Shukla</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Kaushal Kesharawani</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Samrat Varun</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal2" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal2">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Flying machine</h1>
              <p>
                The project was aimed to develop an autonomous flying drone (specifically quadcopter). The drone was equipped 
                with DJI’s Naza M V2 flight controller,  connected to four ESC. The ESC takes the signal from the flight 
                controller and power from the battery and makes the brushless motor spin. The flight controller uses the response 
                of the barometer and the GPS module to detect hight and distance. The barometer was used to set the height of 
                the drone at a fixed distance from the land. The GPS module provides waypoints to the drone to navigate and 
                traverse from one location to another using GPS coordinates. The drone was able to achieve GPS lock and altitude 
                lock during the flight.
              </p>
            </div>
          </div>


          <div class="col-md-2"></div>

          <div class="col-md-5" >
            <p data-modal-target="modal4" >
              <img style="width: 100%; " src="img/projects/5.png">
              <div data-modal-target="modal4" class="card-block">
                <h2 id="protit">Terrace Farming Robot</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/kjyothiswaroop/Terrace-Farming-Robot-InterIITTechMeet8.0"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Harshit Jain</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Srajan Gupta</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Eric John DungDung</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Inamapudi Sai Amith</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Jajula Samrat Varun</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Hrithik Agarwal</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i> Kasina Jyothi Swaroop</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Yelagandula Vasanth Maharshi</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i> Prince Keshri</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal4" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal4">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Terrace Farming Robot</h1>
              <p>
                Problem Statement:
                To Model and Build a Terrace Farming Robot for Hilly Areas.
              </p>
              <p>
                Introduction:
                In Hilly Areas, usually the type of Agriculture practiced is done in Step like Structures and this is called Terrace Farming.
                Evident from the above explanation, it is highly difficult to take animals up the hills to perform the farming taska and the heavy Machinery, well thats out of Question!
                The main motivation behind this project was to help the farmers in the above situation.
              </p>
              <p>
                <h3>The Robot:</h3>
                The Terrace Farming Robot that we built was essentially an Autonomous Stair Climbing Robot which could ascend and descend stair of the height of about 40cm high. 
                The Working principle we chose for the Robot was Chain Slider Mechanism. In the following mechanism the robot has three portions, the front middle and the back where sliders were attched to the middle portions upon which the front and back portions could slide using the chain.
                The Robot was first designed on Solidworks and then fabricated using PVP pipes.
              </p>
              <p>
                The Dimensions of the Robot are 100x70x60 and can perform the following Functionalities:
                <ol style="margin-left: 5px;">
                  <li>Ploughing</li>
                  <li>Seeding</li>
                  <li>Harvesting</li>
                </ol>
              </p>
              <p>
                Components
                <ol style="margin-left: 5px;">
                  <li>Arduino Mega Microcontroller board</li>
                  <li>Hercules Motor Drivers</li>
                  <li>UltraSonic Sensors</li>
                  <li>Color Sensor</li>
                  <li>Planetary Motors(For the Chain Slider Mechanism)</li>
                  <li>DC Motors (For motion and for the plough)</li>
                  <li>Omni Wheels.</li>
                </ol>
              </p>
              <p>
                <h3>Working Of the Robot:</h3>
                <p>
                  The Robot when placed on the top step first detects the yellow color(a part of the problem statement) and 
                  then starts movin forward. In total the robot has 18 Ultra sonic Sensors at various locations. Based on the 
                  distances detected from the walls on either sides the Robot adjusts its speed and once it reaches the area 
                  where the soil is kept it starts ploughing and in the return journey it starts seeding. 
                  The Second time it detects the yellow coloured block the robot descends down the stair. Here ,Once the robot 
                  detects a Red color it again starts moving towards the crops region adjusting speeds and starts Harvesting.
                </p>
              </p>
            </div>
          </div>

          
        

          

          <div class="col-md-5" >
            <p data-modal-target="modal3" >
              <img style="width: 100%; " src="img/projects/3.png">
              <div data-modal-target="modal3" class="card-block">
                <h2 id="protit">MULTIm-8</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/matiyau/MULTIm-8"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Nishad Mandlik</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal3" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal3">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>MULTIm-8</h1>
              <p>
                MULTIm-8 is an automated system for batch programming of up to 16 AT-Tiny85 ICs. It can be operated from any Linux 
                based computer, via USB. It is driven by a python script and an Arduino Uno board. The Uno is used in the ISP 
                configuration, for programming the AT-Tiny85s. MULTIm-8 is capable of hard-coding each IC in a batch, with a 
                unique ID from a user-specified range. This is particularly useful in production of multiple devices. In such 
                cases, the main firmware on all devices needs to be same, but the devices need to identifiable, which is achieved 
                by assigning unique IDs to each of them. The Arduino ISP programs the ICs through SPI. It is also used for 
                selecting the IC to be programmed. Four pins of the Uno are configured as output pins and connected to the 
                inputs of a 4:16 decoder circuit. Those four output pins select the AT-Tiny85 ICs one by one and the program 
                along with the unique ID is flashed onto each one of them.
              </p>
             
            </div>
          </div>


          <div class="col-md-2"></div>


          <div class="col-md-5" >
            <p data-modal-target="modal1" >
              <img style="width: 100%; height: 320px;" src="img/projects/1.png">
              <div data-modal-target="modal1" class="card-block">
                <h2 id="protit">Rubik's Cube Solver</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/matiyau/RuDe"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Nishad Mandlik</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal1" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal1">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Rubik's Cube Solver</h1>
              <p>
                This project initially started off as a python script which out capture the faces of a 2x2x2 Rubik’s Cube and output the 
                moves required to solve it. But soon, it felt incomplete, since the cube was not being physically solved. And thus, 
                I decided to incorporate the electronic and mechanical components and build a bot which would actually solve the cube.
              </p>
             <p>
              The bot can be connected to any computer via USB and the python program can be invoked on it. The bot houses a camera which 
              captures the faces of the cube and an Arduino board which communicates with the python interface and drives the grippers 
              accordingly. The grippers are positioned on the left, right, top and bottom of the cube. The camera is at the back. 
              Each gripper is capable of responding to three types of commands: grip, ungrip and rotate. After the faces are captured, 
              the colours are differentiated. This algorithm is independent of the colour scheme of the cube. The solution is computed 
              on the computer and fed to the Arduino board. For each step, it executes the appropriate commands on the grippers. 
              By using a graph based search, the shortest solution is computed (11 face-turns of the cube, at most) for any given 
              state of the cube.
             </p>
            </div>
          </div>



          <div class="col-md-5" >
            <p data-modal-target="modal5" >
              <img style="width: 100%; background: white; " src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Arduino_Logo.svg/640px-Arduino_Logo.svg.png">
              <div data-modal-target="modal5" class="card-block">
                <h2 id="protit">Image processing and Home automation</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Bronson Naorem</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal5" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal5">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Image processing and Home automation</h1>
              <p>
                Using image processing to detect intruders in real time on raspberry pi and alerting the user.
                IOT base home automation and Power system Management for Uninterrupted Power Supply and Monitoring 
                the fire hazards and therefore prevent it.
              </p>
             
            </div>
          </div>

          <div class="col-md-2"></div>




          <div class="col-md-5" >
            <p data-modal-target="modal13" >
              <img style="width: 100%; background: white;  " src="img/projects/13.png">
              <div data-modal-target="modal13" class="card-block">
                <h2 id="protit">Image colorization</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/25abhishek/Image_colorization"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>froz husain</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>abhishek das</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal13" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal13">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Image colorization</h1>
              <p>
                This project is an implementation of the  research paper "Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification" in Tensorflow. This paper presents a novel technique to automatically colorize grayscale images that combines both global priors and local image features. 
                Based on Convolutional Neural Networks,this deep network features a fusion layer that allows the model to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The network learns both local features and global features jointly in a single framework which can then be used on images of any resolution.
                Moreover, we optimized the code so that it can be trained using transfer learning.</p><p>
                While the model works on any size image, we trained it on 224x224 pixel images and thus it works best on small images. The model mainly produces the chrominance map and then it is rescaled and combined with the original grayscale image for higher quality and final coloured image.
              </p>
            </div>
          </div>



          <div class="col-md-5" >
            <p data-modal-target="modal7" >
              <img style="width: 100%; background: white; " src="img/projects/7.png">
              <div data-modal-target="modal7" class="card-block">
                <h2 id="protit">Automatic Target Shooter</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/hackabit19/Creators"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Ku. Anukriti Rawat</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Neha Modak</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal7" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal7">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Automatic Target Shooter</h1>
              <p>
                Built an automatic gun shooter which can detect a particular person aiming at the centroid of  the detected person 
                and shoot it when it appears in its field of view. Used Viola Jones algorithm for face detection in OpenCv and 
                LBPH algorithm for face recognition. Arduino UNO was used as the development board.
              </p>
             
            </div>
          </div>

          <div class="col-md-2"></div>


          <div class="col-md-5" >
            <p data-modal-target="modal8" >
              <img style="width: 100%; background: white; height: 350px; " src="img/projects/8.png">
              <div data-modal-target="modal8" class="card-block">
                <h2 id="protit">Epileptic Seizure Detection and Alert Device</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/kjyothiswaroop/Seizure-glove"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Parth Khanna</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Kasina Jyothi Swaroop</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Suddunuri Sandeep</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal8" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal8">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Epileptic Seizure Detection and Alert Device</h1>
              <p>
                Epilepsy is a central nervous system (neurological) disorder in which brain activity becomes abnormal, causing 
                seizures or periods of unusual behaviour, sensations, and sometimes loss of awareness. This conditions abrupt 
                episodes called Seizures where the patient loses control on self and this condition if not assisted immediately 
                may be fatal.
              </p>
             <p>
              The focus is on making the lives of the Patients caretakers as studies have shown that most of the times the 
              caretakers are parents and these conditions causes stress and symptoms of PTSD.
              We constructed a wearable wrist band which will record sEMG signals and Accelerometry with the help of sensors such 
              as EMG Muscle Sensor Module and a triaxial accelerometer embedded into the band. Using this information, we shall 
              predict epileptic seizures and notify the caregiver in case a seizure actually occurs with the help of an Android 
              application. The application will notify the caregiver such as a parent that an episode is taking place and also 
              the exact location of the patient on Google maps. The solution also consists of a continuous low power heart rate 
              monitoring system by Motion Capturing System (IMUs). The band will also consist of an audio module, which shall 
              provide instructions on how to deliver first aid to the patient, for nearby people to help, in case the caregiver 
              is far away. The application will also feature a database where the patient history can be uploaded for medical 
              use including sleep time cycle. It will also include First Aid instructions to be followed during the seizure such 
              as turning of the head to prevent choking, as well as the location of the nearest hospitals, in case of an emergency.
              We obtained the dataset from Patients Data from Health Centre inside the campus, which will be used to train the 
              necessary Machine Learning/Deep Learning model. The model will predict the seizures activity by using the real-time 
              data collected from sensors and will use that as test data to classify the activity as seizures or not. The IoT 
              based alarming system will then inform the caretakers via Android application about the activity with certain other 
              information as detailed earlier.
             </p>
             <p>
              <h3>Technology Stack:</h3>
              EMG sensor and electrodes, MPU6050 accelerometer, ESP8266 Wi-Fi module, GPS module, Python Language, Deep Learning, 
              Keras, Firebase, Android Studio, Arduino IDE, PLX-DAQ, Microsoft Office.
             </p>
            </div>
          </div>



          <div class="col-md-5" >
            <p data-modal-target="modal9" >
              <img style="width: 100%;  " src="img/projects/9.png">
              <div data-modal-target="modal9" class="card-block">
                <h2 id="protit">Object Detection and Tracking For Aerial View</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Nishad Mandlik</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Sai Ruthvik</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Sai Sumanth</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal9" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal9">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Object Detection and Tracking For Aerial View</h1>
              <p>
                Aerial Imaging plays an important role in military surveillance as well as  in search and rescue operations. 
                This project, titled “Object Detection and Tracking for Aerial View”, aims to develop an autonomous navigation 
                system for aerial vehicles. Sensors like inertial measurement unit and GPS help in achieving flight stability and 
                immunity to drift due to winds. An efficient embedded system needs to be designed in order to incorporate neural 
                networks and image processing techniques.
              </p>
              <p>
                Autonomous navigation demands a real-time spatial (object detection) and temporal (object tracking) analysis of 
                the video feed. The You Only Look Once (YOLO) algorithm based on the Darknet Neural Network has been used for 
                object detection. Channel and Spatial Reliability Tracker (CSRT) and Kalmann Filters have been used for object 
                tracking. Occlusions have been handled by matching features extracted from the object being tracked. The neural 
                network has been trained on the CARPK Dataset which contains aerial images and annotations for parked cars.
              </p>
            </div>
          </div>

          <div class="col-md-2"></div>


          <div class="col-md-5" >
            <p data-modal-target="modal10" >
              <img style="width: 100%; " src="img/projects/10.png">
              <div data-modal-target="modal10" class="card-block">
                <h2 id="protit">IRis</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/matiyau/IRis"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Nishad Mandlik</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Sai Ruthvik</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Sai Sumanth</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Neelesh Kumar Yadav</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Asutosh Rout</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal10" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal10">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>IRis</h1>
              <p>
                Audience Response Systems are deployed in a wide variety of applications these days. The simplest of such 
                wireless systems use Radio Frequency for communication. The others work on Bluetooth, WiFi or Internet. One major 
                drawbacks with such architecture is considerable hardware cost.
              </p>
              <p>
                "IRis" as we call it, has been mindcrafted to tackle this very problem. We conceptualized IRis with the goal 
                of making digital education a reality even in most remote areas of nation. We have modified the technology 
                already in use, to develop a low cost alternative. At the heart of IRis lies an IR communication architecture 
                instead of RF. Needless to say, IRis results in lower hardware cost and power consumption than its RF counterparts.
              </p>
              <p>
                Coming to the technicalities of the project, IRis Slaves are driven by Attiny85 microcontrollers. IRis employs 
                Time Division Multiplexing (TDM) using Master-Slave protocol to avoid interference and ensure hassle free 
                communication. Presently, IRis can offer support to 63 polling devices, each hardcoded with a unique identity 
                number
              </p>
            </div>
          </div>



          <div class="col-md-5" >
            <p data-modal-target="modal12" >
              <img style="width: 100%; background: white;  " src="img/projects/12.png">
              <div data-modal-target="modal12" class="card-block">
                <h2 id="protit">Smart Attendance System</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/jayant-ism/Smart-attendance"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>ayant Anand</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Ku. Anukriti Rawat</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Neha Modak</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Sayantani Bhattacharya</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal12" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal12">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Smart Attendance System</h1>
              <p>
                A system is developed wherein as a student enters the class in the first period, his/her face will be recognized 
                by the CCTV camera and the student will be registered as present in the class. The name of the student will be 
                displayed on the LCD screen(placed above the blackboard), so that he/she can know that their attendance has been 
                recorded. Immediately, their parents will also receive a message that their wards have reached the school, and 
                thus students won’t be able to bunk the school and also will ensure parents that they have safely reached the 
                school premises. Once the attendance has been marked in the first period, it will be notified (saved), so that 
                it can help other teachers for verification. The number of students currently present in the class will continuously 
                be displayed on the display board (LCD). For marking the presence of the wards in the class at proper time it is 
                required that they get their attendance marked before the entry of the teacher(teacher may provide them 10 
                minutes relaxation). The face of the teacher will also be recognized and once the teacher enters the class, the 
                camera will be closed and none of the student will get attendance after that. The total student tally, will vary 
                depending on the current number of students present inside the classroom. The student’s entry and exit will be 
                monitored and tracked to keep updating the number of students present in the class at the moment. Each day’s 
                information regarding the total number of students present and absent will be stored and maintained in a 
                database(firestore database). If a particular student is falling short of attendance for example less than 
                75 percent then, a message will be conveyed regarding it to their parents. The class teacher will be given Admin 
                Access to the portal (wherein attendance is being uploaded), in case of any query or issue related to attendance 
                one can directly contact their respective class teacher.
              </p>
              <p>
                <ul>
                  <li>Software Used: postgresql, Python 3.6, Python IDLE.</li>
                  <li>Python libraries: Dlib, Tensorflow, Django, OpenCv.</li>
                  <li>Hardware used: Node MCU, LCD module, Web Camera.</li>
                </ul>
              </p>
            </div>
          </div>



          <div class="col-md-2"></div>


          <div class="col-md-5" >
            <p data-modal-target="modal11" >
              <img style="width: 100%; background: white;  " src="img/projects/11.png">
              <div data-modal-target="modal11" class="card-block">
                <h2 id="protit">Ergonomic Crutches</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Eric John</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Prince Kunal </li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Anshul Kashyap</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal11" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal11">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Ergonomic Crutches</h1>
              <p>
                These ergonomic crutches were designed and fabricated during the Inter-IIT BeTIC Medical Innovation Challenge 
                2018 held in IIT Bombay.
              </p>
              <p>
                Most of the crutches available in the market have many drawbacks of its own. The challenge was to design a new 
                set of ergonomic crutches which can solve the problems like a disabled person with regular crutches cannot climb 
                stairs easily. No hands-free movements are possible, making it difficult to grab anything else, prolong use of 
                crutches cause muscle pain and even muscle paralysis, portability issues and many more.

              </p>
              <p>
                The new design of the ergonomic crutches uniquely tackle this problems. The armrest is at an angle to distribute 
                the load evenly along the forearm. The pivot-less cuff allows full range of motion while maintaining one-piece 
                design. The handle has an adjustable length which slides into the armrest providing a single solution for people 
                with different arm length. The thigh cuff offers additional support to the body by reducing body load directly 
                from the forearm and redistributing it through the thigh and transferring back to the ground. This reduces the 
                stress from the forearm, and it prevents hand segs. Since this crutches design provides support at two points, 
                a person can easily climb the stairs with comfort and ease without losing balance. Its design permits the person 
                to fold it to half of its length, which makes it easy to store and portable. Through adjustment, pin person can 
                adjust the height of the clutches for his/her needs
              </p>
            </div>
          </div>




          <div class="col-md-5" >
            <p data-modal-target="modal6" >
              <img style="width: 100%; background: white; " src="img/projects/6.png">
              <div data-modal-target="modal6" class="card-block">
                <h2 id="protit">Face Mask Detection </h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/ParthKhanna07/Mask-detection-Keras"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Parth Khanna</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal6" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal6">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Face Mask Detection </h1>
              <p>
                Face Mask Detection uses computer vision to detect the face and a CNN algorithm then classifies the face with or 
                without the mask. The project classifies whether the person is with/ without a mask by placing a bounding box on 
                the face. Used HAAR cascading in OpenCV to detect the face, and then tested the generated real-time video with 
                a Machine learning model. The model is built and manually-trained with Tensorflow.keras and uses Sequential 
                Model with a combination of Convolution layers, Pooling layers, Dense layers, and Drop-out Layers as well. 
                The model provides an accuracy of more than 95% on both Training as well as the Validation Set. Used the 
                publically available mask detection dataset from Kaggle. This project can be useful for public places of 
                entries such as railway stations, metro stations, shopping malls, etc.
              </p>
             
            </div>
          </div>





          <div class="col-md-2"></div>



          <div class="col-md-5" >
            <p data-modal-target="modal14" >
              <img style="width: 100%; background: white;  " src="img/projects/14.png">
              <div data-modal-target="modal14" class="card-block">
                <h2 id="protit">PCAPTCHA</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/harshv47/SIH-20-qual"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Pradyumna Gupta</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Harsh Vardhan</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Sourav Sahu </li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Utkarsh Deep</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal14" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal14">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>PCAPTCHA</h1>
              <p>
                An alternate for the current CAPTCHA that doesn't need the user to interact and solve to prove itself human.
                The software is divided into two parts, one runs on your browser and the other is on the website. The client side does all the analysis and sends the probability of the user being a robot to the website which can then take any action they want.
              </p>
              <p>
                The client side determines the probability using three different approaches:
                <ul>
                  <li>An average human's internet journey consists almost entirely of the top 200 websites. So there is a high probability that the user contains these websites. The extension matches these and produces a result.</li>
                  <li>A bot generally works by using the x-path of an element to interact with it. So, this part changes the x-path by adding random elements in between.</li>
                  <li>This part analyses the mouse movement of the user on the page and categorizes it as a bot or a human by using an LSTM model.</li>
                </ul>
              </p>
              <p>The final probability is calculated using the probabilities in the first and the third part.</p>
            </div>
          </div>





          <div class="col-md-5" >
            <p data-modal-target="modal16" >
              <img style="width: 100%; background: white;  " src="img/projects/16.png">
              <div data-modal-target="modal16" class="card-block">
                <h2 id="protit">ParaShoot</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/matiyau/ParaShoot"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Asutosh Rout</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Neelesh Kumar Yadav</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Nishad Mandlik</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Rahul Sridhar</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal16" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal16">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>ParaShoot</h1>
              <p>
                Intrusion attempts across land borders are frequently faced by our country. In the process of defending such threats, several brave men have sacrificed their lives, even during peace time. Troops are even being deployed at locations with extreme climatic conditions. Every year, many soldiers succumb to these harsh conditions. It is the need of the hour, to incorporate an automated system which will assist in border security and help in saving lives of our security personnel.
              </p>
              <p>
                Defense Research and Development Organization (DRDO) has developed an autonomous multi-terrain vehicle called Daksh, which is capable of defusing bombs and neutralizing chemical weapons. On similar lines, we conceived ParaShoot, an autonomous vehicle for detecting human presence and eliminating the threat if necessary. Parashoot employs a dual camera system for precisely detecting the position of an object in 3D Cartesian coordinate space. The frames from both cameras are separately 
                processed using OpenCV and humans are detected. By correlating the features between the two frames and applying some geometric calculations the coordinates of the intruder are computed. With the coordinates, a servo motor mounted on the bot will rotate to point the laser (gun or tranquilizer in practical scenario) at the target. Multiple IR emitters around both cameras make the bot capable of operations at night and in low visibility conditions as well.
              </p>
            </div>
          </div>


          <div class="col-md-2"></div>

          
          
          <div class="col-md-5" >
            <p data-modal-target="modal15" >
              <img style="width: 100%; background: white;  " src="img/projects/15.png">
              <div data-modal-target="modal15" class="card-block">
                <h2 id="protit">Making Machine Converse Better</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/deepak4669/Conversational-Agents"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Deepak goyal</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Deepak Agrawal</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Nitanshi Mahajan</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal15" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal15">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Making Machine Converse Better</h1>
              <p>
                An alternate for the current CAPTCHA that doesn't need the user to interact and solve to prove itself human.
                The software is divided into two parts, one runs on your browser and the other is on the website. The client side does all the analysis and sends the probability of the user being a robot to the website which can then take any action they want.
              </p>
              <p>
                The client side determines the probability using three different approaches:
                <ul>
                  <li>An average human's internet journey consists almost entirely of the top 200 websites. So there is a high probability that the user contains these websites. The extension matches these and produces a result.</li>
                  <li>A bot generally works by using the x-path of an element to interact with it. So, this part changes the x-path by adding random elements in between.</li>
                  <li>This part analyses the mouse movement of the user on the page and categorizes it as a bot or a human by using an LSTM model.</li>
                </ul>
              </p>
              <p>
                The final probability is calculated using the probabilities in the first and the third part.
              </p>
            </div>
          </div>




          <div class="col-md-5" >
            <p data-modal-target="modal18" >
              <img style="width: 100%; background: white;  " src="img/projects/18.png">
              <div data-modal-target="modal18" class="card-block">
                <h2 id="protit">UltraBitBot</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/JaySharma1048576/BotISM-Techkriti-19-"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Kartike Kishore</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Kartike Saini</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Karan Tyagi</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal18" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal18">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>UltraBitBot</h1>
              <p>
                In this project, we used an Arduino Uno to make an obstacle avoiding line follower bot which uses 2 IR sensors to detect the black line to follow and an ultrasonic sensor to check for the obstacle and it stops if it encounters an object, within a particular range, in the front of the bot.
              </p>
              <p>
                In chassis we  had installed two DC motors , two wheels attached to the  Motos and one caster wheel.
                We also had two IR SENSOR on each side i.e. right and left.
                Ultrasonic sensor was at centre and front part of the chassis.
                We used differential drive to change its direction.
                IR sensor sends signals and receives the bounced back / reflected signals and detects whether it is black path to follow or other path to change , as the black colour absorbs most of the light signals.
              </p>
              <p>
                For detecting obstacles we used an ultrasonic sensor which works on the same principle and gives us the distance between the bot and obstacle . Hence we can stop and wait till it gets out of our path.
              </p>
            </div>
          </div>


          <div class="col-md-2"></div>


          <div class="col-md-5" >
            <p data-modal-target="modal17" >
              <img style="width: 100%; background: white;  " src="img/projects/17.png">
              <div data-modal-target="modal17" class="card-block">
                <h2 id="protit">BotISM</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/JaySharma1048576/BotISM-Techkriti-19-"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Jay Sharma</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Swaraj jain</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Jai Gupta</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i> A. V. Jefferson</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal17" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal17">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>BotISM</h1>
              <p>
                This bot was specifically made for the event IARC of Techkriti '19. The problem statement was to solve a maze with additional functions like reading data from black and white grids called nodes, controlling speed of the bot as per the data or choosing the right path using the data, stopping at red signals and obstacles. We used 2 IR sensors to solve the maze using the 
                left-hand algorithm. Four IRs were used to read data from nodes and 4 more were used to distinguish a node from a path. We used an RGB colour sensor to detect Red LED signals and an Ultrasonic sensor to detect obstacles. We used a hall-effect sensor and magnets to calculate the bot's speed and adjusted it to the desired speed using PWM. Finally, a compass-sensor 
                was used to calculate the angle turned by our bot and turn until we reached the correct angle where our correct path was. We also used an LCD display to display the values we read from a node. 
              </p>
            </div>
          </div>




          <div class="col-md-5" >
            <p data-modal-target="modal22" >
              <img style="width: 100%; background: white;  " src="img/projects/22.png">
              <div data-modal-target="modal22" class="card-block">
                <h2 id="protit">Vacuum cleaner bot</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Akanksha gupta</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal22" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal22">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Vacuum cleaner bot</h1>
              <p>
                It has a ultrasonic sensor to detect and avoid obstacles,and thus traversing the floor area.The vacuum cleaner part has a motor to create suction and clean the floor area while the bot move covering the entire area. it has a Arduino to control the ultrasonic sensor and a motor driver which was connected to motors and wheels for the movement of bot
              </p>
            </div>
          </div>





          <div class="col-md-2"></div>


          <div class="col-md-5" >
            <p data-modal-target="modal19" >
              <img style="width: 100%; background: white;  " src="img/projects/19.png">
              <div data-modal-target="modal19" class="card-block">
                <h2 id="protit">FPV Drone</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Zeeshanul Hoda</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal19" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal19">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>FPV Drone</h1>
              <p>
                A lightweight 230mm drone made of carbon fiber and with a first person view camera capable of reaching high speeds. The weight of the drone is nearly 600gms. The camera transmits live video feed to the drone pilot with a latency of less than 20ms. The drone can be controlled within a range of 1km with good video feed. There is an additional GoPro camera to record HD footage. 
                The motors can provide a maximum individual their of 1kg and are powered individually by a 40 amp electronic speed controller. This gives the drone a top speed of nearly 150 kmph and an acceleration of 0-100kmph in 1 second. The flight of the drone is controlled by an F4 flight controller, and the whole drone is powered by a 4 cell li-po battery capable of providing a maximum of 150 amps of current. The total flight time is nearly 5 minutes.
              </p>
            </div>
          </div>




          <div class="col-md-5" >
            <p data-modal-target="modal21" >
              <img style="width: 100%; background: white;  " src="img/projects/21.png">
              <div data-modal-target="modal21" class="card-block">
                <h2 id="protit">Smart Irrigation System</h2>
                <a id="progit" href="https://github.com/swaraj-jain/SMART-IRRIGATION-SYSTEM"><i class="fab fa-github" ></i></a>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Swaraj jain</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Surya Prakash Mishra</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal21" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal21">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Smart Irrigation System</h1>
              <p>
                <h3>Problem</h3>
                Several studies and surveys have shown that the performance of irrigation practices and equipment, especially in the uniformity of water application  ,is still too low .This is due to farmers lacking the management skills to manage their irrigation systems properly due to which  consequences result in reductions in crop yield and a waste of water resources .
              </p>
              <p>
                <h3>Solution</h3>
                To improve the irrigation performance ,it is necessary not only to promote the implementation of irrigation scheduling methods but concurrently improve system designs and enhance farmers skills to control and manage their irrigation system more efficiently during its operation. 
                Thus ,The smart irrigation system is an IOT based device which is capable of automating the irrigation process by analyzing the moisture of soil and the climatic condition (like raining).
                Also the farmer wouldn’t have to go again and again to fields and even control the irrigation of fields if he out of town or so by webpage linked to the microcontroller.
                Hence ,Access to this modern technology could act as a boost to productivity and farming technology
              </p>
              <p>
                <h3>Approch</h3>
                In this project,we will command the the NodeMCU microcontroller through a webpage to control the motor and the rest irrigation process will be automatically controlled by NodeMCU.
              </p>
            </div>
          </div>




          <div class="col-md-2"></div>


          <div class="col-md-5" >
            <p data-modal-target="modal20" >
              <img style="width: 100%; background: white;  " src="img/projects/20.png">
              <div data-modal-target="modal20" class="card-block">
                <h2 id="protit">Petrol Scam Detection</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/ParthKhanna07/HackFest2k19"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Parth Khanna</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal20" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal20">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Petrol Scam Detection</h1>
              <p>
                We have a Hardware component installed at each persons fuel tank . The hardware component consists of a FlowRate Sensor A NodeMCU module and a Weight Sensor. This sensor measures the amount of liquid 
                flowing through a particular cross-section as well as the mass of the liquid flown. Using this data an OTP is generated which sent to the owner of the vehicle . Using this OTP the user can view his details from the PetroBYTES Website The website then asks for the amount of petrol that he actually wanted and then calculates the amount of money he has been doped of. The user can next select the address of the petrol pump from the dropdown and submit this data to a database. Using this data We aim to predict the average FLowrate Error and Adulteration for each Petrol Pump using Linear Regression Model. This data will be Publicly Available and hence the user can decide which is the best petrol pump for him.It also creates pressure on the petrol pump owners to act responsibly since the entire system becomes transparent.
              </p>
              <p>
                The system has been Updated For Five Petrol Pumps namely A to E Additional Datasets have been used for correctly doing the data Analysis
              </p>
            </div>
          </div>



          <div class="col-md-5" >
            <p data-modal-target="modal24" >
              <img style="width: 100%; background: white;  " src="img/projects/24.png">
              <div data-modal-target="modal24" class="card-block">
                <h2 id="protit">keypad door unlock</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Chepuri V S S Santosh Kumar</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal24" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal24">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>keypad door unlock</h1>
              <p>
                This project is a model of a locker where it is opened when correct key is pressed.
                when wrong key is pressed , buzzer will be blown and red led is glown indicating the wrong key . When correct key is pressed , green led is glown 
                and door is opened. First,I took a card board and made a model of door and placed all parts on it on suitable places.I took jumper wires and connected 
                keypad to 1,2,3,4,5,6,7,8 pins of arduino . I connected RS , E , D4 , D5 , D6 , D7 pins of  lcd screen to A0 , A1 , A2 , A3 , A4 , A5 pins of arduino 
                respectively .VSS , RW,cathode of lcd are connected to ground pin of arduino and VDD , anode are connected to 5V pin of arduino using breadboard and it's VE 
                pin is connected to potentiometer . Servo motor, piezo buzzer, red led, green led are attached to 10,13,11,12 of arduino respectively.  By using usb cable 
                I uploaded code into arduino. 
              </p>
            </div>
          </div>


          <div class="col-md-2"></div>


          
          <div class="col-md-5" >
            <p data-modal-target="modal23" >
              <img style="width: 100%; background: white;  " src="img/projects/23.png">
              <div data-modal-target="modal23" class="card-block">
                <h2 id="protit">Self balancing Bot</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/kartikekishore/self-balancing-bot"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Kartike Kishore</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal23" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal23">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Self balancing Bot</h1>
              <p>problem statement :- Make a bit that balances itself on two wheels.</p>
              <p>
                <ul>
                  <li>Mpu6050 (gyroscope and accelerometer)</li>
                  <li>L298 motor drivers</li>
                  <li>Bo-motors( two motors )</li>
                  <li>Arduino Uno</li>
                </ul>
              </p>
              <p>
                Mpu6050 is an gyroscope and accelerometer.It gives the position and values of it's position in 3 axis ,and by proper use of PID CONTROL we can easily make it balance itself. 
                Here PID stands for proportional integral derivative.
                 PID control provides a continuous variation of output within a control loop feedback mechanism to accurately control the process, removing oscillation and increasing process efficiency.
                PID is used in many of our day to day equipments like drone.
                Drone also uses PID control method to take off, remain in mid air,move ,etc.
              </p>
              <p>
                In this project I made a self balancing Bot in some simple steps.
                At first I made the chassis .Then by installing the components .
                Doing the circuits properly.
                Followed by coding and testing.
                Finally it worked just fine.
                This project was very interesting and fun to make.

              </p>
            </div>
          </div>




          <div class="col-md-5" >
            <p data-modal-target="modal26" >
              <img style="width: 100%; background: white;  " src="https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRoVckIUjRme_U3NwRg4zWnofOS8NZS01RNeA&usqp=CAU">
              <div data-modal-target="modal26" class="card-block">
                <h2 id="protit">Handwritten  digit  generator</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/jayant-ism/digit-generator.git"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Jayant Anand </li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal26" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal26">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Handwritten  digit  generator</h1>
              <p>
                In this project, a Machine  Learning model that converts noise or a distorted image into an image of a handwritten digit was designed using DCGAN.
                Tensorflow is used to design this model. This model can be used to increase the quality of images containing handwritten digits and thus can find an application in converting old handwritten records into digital format.
              </p>
              <p>
                The project consists of two models - a generator and an image classifier.
                MNIST dataset of images of handwritten digits was used to train the classifier.
                During training, a random image is fed into the generator model and the output is fed to the classifier, 
                the classifier is then trained to identify all the images from the dataset as the handwritten digits, and 
                non-handwritten digits to that of the image generated from the generator. The generator is then trained to 
                fool the classifier.
              </p>
              <p>
                After training the model it was able to enhance the quality distorted images of handwritten digits. Moreover, it was also able to generate new images of the same.
                This technology can be used to train the models for whom a large quantity of dataset is unavailable, usually, we can train a DCGAN for that particular dataset and 
                by feeding it noise new dataset will be generated which can be further used in training other models.
              </p>
            </div>
          </div>




          <div class="col-md-2"></div>


          <div class="col-md-5" >
            <p data-modal-target="modal25" >
              <img style="width: 100%; background: white;  " src="img/projects/25.png">
              <div data-modal-target="modal25" class="card-block">
                <h2 id="protit">Buzz wire game</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Karan Tyagi</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal25" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal25">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Buzz wire game</h1>
              <p>
                The main objective was to create a simple, easy and fun game with a little touch of technology. The components required are a conducting wire, lcd display, a buzzer, an Arduino, potentiometer, and some bunch of LEDs and jumping wires. And yes, don't forget the switch. Start with making connections. Try it in that way so that every time your loop or key touches the wire, buzzer beeps and LEDs flash. Keep a count on no. of times this happen, and you can flash the final score on display using Arduino.
              </p>
              <p>
                I set an initial score of 100 at the beginning, and with every touch, you get -1. So if you complete the game after beeping the buzzer 10 times, your score would be 90.
              </p>
            </div>
          </div>

         

          <div class="col-md-5" >
            <p data-modal-target="modal29" >
              <img style="width: 100%; background: white;  " src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Arduino_Logo.svg/640px-Arduino_Logo.svg.png">
              <div data-modal-target="modal29" class="card-block">
                <h2 id="protit">RoomTracker</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Akshat Jain</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal29" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal29">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>RoomTracker</h1>
              <p>
                Room tracker is a small simulation of large systems or bots which are used to map or follow a particular path and send back location to servers and thus helps in navigation. In this project, I used a 4 wheeled chassis to develop a small bot . It uses the Arduino UNO development board to control  the movement of wheels mounted on chassis.  A node MCU ESP 8266 WiFi module is used to send and receive location data of the bot in the form of coordinates of the path on which it was moving. Here I,used Kalman Filter algorithm, to reduce the noise in the data to be sent.
                Then, I process the coordinates received from our bot on the adafruit.io portal and thus judged real-time velocity and acceleration of the bot. 
                I further plan to use an app to navigate the bot through the path drawn by me. 
              </p>
            </div>
          </div>


          <div class="col-md-2"></div>



          <div class="col-md-5" >
            <p data-modal-target="modal28" >
              <img style="width: 100%; background: white;  " src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Arduino_Logo.svg/640px-Arduino_Logo.svg.png">
              <div data-modal-target="modal28" class="card-block">
                <h2 id="protit">JOYeux</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="http://github.com/matiyau/JOYeux/"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Nishad Mandlik</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal28" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal28">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>JOYeux</h1>
              <p>
                JOYeux (French : Happy) is a plug and play USB Human Interface Device. It has been built on Arduino Uno. Mouse functionality has been 
                achieved with a joystick and three push buttons. User can also provide numeric input through the 0-9 keypad. The keypad has keys 
                for +,-,*,/,(,). thereby providing support for typing for arithmetic operations. Additionally, another push button is present, 
                which acts like a modifier key for the keypad. With this, JOYeux is able to perform several other routine functions. 
                It can perform arrow based up/down/left/right navigation as well as back and forward functions in a browser. It also 
                supports media volume control with modifier combinations for volume up, volume down and volume mute. Support for 
                control keys like Enter and Esc has also been implemented. Other modifier combinations include editing functions like 
                backspace, delete, cut, copy, paste, select all and save file. JOYeux has been designed according to the HIDprotocol 
                specification which makes it compatible with all operating systems.
              </p>
            </div>
          </div>




          <div class="col-md-5" >
            <p data-modal-target="modal30" >
              <img style="width: 100%; background: white;  " src="img/projects/30.png">
              <div data-modal-target="modal30" class="card-block">
                <h2 id="protit">Knowledge Based Conversational Agents</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/deepak4669/Wizard-Of-Wikipedia"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Deepak Goyal</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal30" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal30">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Knowledge Based Conversational Agents</h1>
             <p>
               The project was based on Open-Domain Conversational Agents. Specifically, it involved including recalled knowledge alongside the context for generating the next utterance. The open-domain dialog systems should have knowledge about the topic they are going to converse in, the project aims to include this knowledge alongside the history of the dialogues as the single context for the next utterance. Variations of the Transformer model and Memory Networks were used in developing various model classes. Specifically, two classes of models were developed 1. Retrieval Based Models, 2. Generative Models. Retrieval Models retrieves the nearest utterance from the candidate utterances based on scores generated from the Transformer model. Generative Models generates the next utterance given the context(The previous Dialog history+Knowledge Sentences). We used the recently released Wizard Of Wikipedia Dataset[ICLR 2019] for training and testing purposes. We evaluated our models on a variety of automatic evaluation metrics like Perplexity, Unigram F1, etc.
             </p>
            </div>
          </div>


          <div class="col-md-2"></div>


          <div class="col-md-5" >
            <p data-modal-target="modal27" >
              <img style="width: 100%; background: white;  " src="img/projects/27.png">
              <div data-modal-target="modal27" class="card-block">
                <h2 id="protit">PixTex (PIXelated TEXt)</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/matiyau/PixTex"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Nishad Mandlik</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal27" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal27">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>PixTex (PIXelated TEXt)</h1>
              <p>
                PixTex is a C implementation of steganography, for hiding text inside a bitmap image. Each pixel can conceal one ASCII character. In order to avoid any visible change in the host image, the text data should be embedded into the least significant positions of the pixels. A pixel consists of three independent dimensions namely, Blue, Green and Red. The least significant positions of these dimensions are modified for storing ASCII codes. The algorithm has been designed such that, the absolute difference between any of the B, G and R value of any pixel in the modified image, and that in the original one is always less than or equal to 4. Thus, practically, no change is perceived in the image after hiding the text. To improve the reliability of the algorithm, start and stop codes are appended to the text data. These reduce the chances of false detection of text in the image. The program has support for hiding new text, appending to the already hidden text, reading the hidden text and erasing the text hidden in the image.
              </p>
            </div>
          </div>


          <div class="col-md-5" >
            <p data-modal-target="modal31" >
              <img style="width: 100%; background: white;  " src="img/projects/31.png">
              <div data-modal-target="modal31" class="card-block">
                <h2 id="protit">Smart Lock</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <a id="progit" href="https://github.com/KratiAnu/Smartlock"><i class="fab fa-github" ></i></a>
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Ku. Anukriti Rawat</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal31" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal31">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Smart Lock</h1>
              <p>
                Built a bluetooth controlled smart door in which an application was developed for fastening the latch. A 90g servo motor arm was attached with the latch.
              </p>
              <p>
                The servo arm was connected to the slider lock and will move between 0 and 180 degrees to lock and unlock the door, using commands it gets from the Bluetooth device. An HC-05 Bluetooth module was used.
                As the Bluetooth is turned on and connected to the Bluetooth module, the motion of the motor can be controlled via moving the sliders on the developed android application to open and close the door. The android application was built using App Inventor. Arduino UNO was used as the development board.
              </p>
            </div>
          </div>

          <div class="col-md-2"></div>



          <div class="col-md-5" >
            <p data-modal-target="modal33" >
              <img style="width: 100%; background: white;  " src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Arduino_Logo.svg/640px-Arduino_Logo.svg.png">
              <div data-modal-target="modal33" class="card-block">
                <h2 id="protit">Robowar Bot</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Jalaj Tripathi</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Ayan Prakash</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Srajan Gupta</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Harshit Jain</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Anshul Agarwal</li>
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Rajat Pal</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal33" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal33">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Robowar Bot</h1>
              <p>
                This project was developed during a 36-hours long hackathon, Hackfest ‘19. The aim of this project was to develop the prototype of a technology to stabilize two-wheeled vehicles to reduce the on-road accidents. The prototype works on Reaction wheel mechanism which uses a high torque motor and a fly wheel to control the posture of the vehicle. A complimentary filter was used to calculate the accurate inclination of the vehicle using the accelerometer and gyroscope readings from the MPU6050. The deviation from vertical axis was fed to the PD controller which controlled the high torque DC motor to generate a restoring torque. This restoring torque maintained the vertical position of the two-wheeler. The chassis was 3D printed using PLA. This project won the 2nd Runners-up in the competition.
              </p>
            </div>
          </div>




          <div class="col-md-5" >
            <p data-modal-target="modal32" >
              <img style="width: 100%; background: white;  " src="img/projects/32.png">
              <div data-modal-target="modal32" class="card-block">
                <h2 id="protit">Drive-Assist Technology</h2>
                <hr style="color: #d3d3d3; margin-top: 5px; margin-bottom: 2rem;">
                <ul >
                  <li id=protiti><i id="symper1" class="fas fa-user"></i>Mayank Kumar Gupta</li>
                </ul>
                <p class="btn btn-primary" data-modal-target="#modal32" style="margin-left: 38%;">MORE INFO</p>
              </div>
            </p>
          </div>
          <div class="modal"  id="modal32">
            <div class="modal-content">
              <span class="modal-close" style="font-size: 3.3rem; margin-right: 8px; margin-top: 4px; color: #d3d3d3;">&times;</span>
              <h1>Drive-Assist Technology</h1>
              <p>
                This project was developed during a 36-hours long hackathon, Hackfest ‘19. The aim of this project was to develop the prototype of a technology to stabilize two-wheeled vehicles to reduce the on-road accidents. The prototype works on Reaction wheel mechanism which uses a high torque motor and a fly wheel to control the posture of the vehicle. A complimentary filter was used to calculate the accurate inclination of the vehicle using the accelerometer and gyroscope readings from the MPU6050. The deviation from vertical axis was fed to the PD controller which controlled the high torque DC motor to generate a restoring torque. This restoring torque maintained the vertical position of the two-wheeler. The chassis was 3D printed using PLA. This project won the 2nd Runners-up in the competition.
              </p>
            </div>
          </div>

          

          




        </div>

      </div>

  <!--==============================================================================-->




      

    

    <!--Navbar-->
     <!--===========================================================-->
     <header>
      <a href="index.html" class="logo"><img class="navlogo" src="img/logo1.png"></a>
      <nav>
        <ul>

          <li><a href="index.html" id="part" >Home</a></li>
                  <li><a href="members.html" id="part" >Meet the Team</a></li>
                  <li><a href="ach.html" id="part" >Achievements</a></li>
                  <li><a href="alumini.html" id="part" >Alumni</a></li>
                 
              
          <li><a href="project.html" >Projects</a></li>
         
              <li><a href="news.html" id="part" >News</a></li>
              <li><a href="blogs.html" id="part" >Blogs</a></li>
              
          <li><a href="activity.html" >Activity</a></li>
          <li><a href="gallery.html" >Gallery</a></li>
          <li><a href="index.html#codi" >Contact</a></li>
        </ul>
      </nav>
      <div class="menu-toggle"><i class="fa fa-bars" aria-hidden="true"></i></div>
    </header>
  <!--===========================================================-->

        <!--FOOTER-->
      <!--=================================================-->

      

      <div id="sym">
        <ul style="margin-bottom: 0em;">
            <li>
              <a add target="_blank" href="https://www.facebook.com/roboism/">
                <i class="fab fa-facebook-f icon"></i>    </a>
            </li>
            <li>
              <a add target="_blank" href="https://www.youtube.com/channel/UCYkjMmf1DwTBoRYK49hfSZg"><i class="fab fa-youtube icon"></i></a>
            </li>
            <li>
              <a  add target="_blank" href="https://www.linkedin.com/company/roboism/"><i class="fab fa-linkedin-in icon"></i></a></li>
            <li>
            <a  add target="_blank" href="https://github.com/RoboISM"><i class="fab fa-github icon"></i></a></li>
          </ul>
      </div>
    
    <div style="color: white; background: black; height: 6%;"><p id="copyr" style="margin-bottom: 0em;">© 2020 Copyright : ROBOISM</p></div>

    <!--===========================================================-->
    
<script src="https://code.jquery.com/jquery-3.3.1.js" integrity="sha256-2Kok7MbOyxpgUVvAk/HJ2jigOSYS2auK4Pfzbm7uH60=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script>
    const modalTriggerButtons = document.querySelectorAll("[data-modal-target]");
const modals = document.querySelectorAll(".modal.div");
const modalCloseButtons = document.querySelectorAll(".modal-close");

modalTriggerButtons.forEach(elem => {
  elem.addEventListener("click", event => toggleModal(event.currentTarget.getAttribute("data-modal-target")));
});
modalCloseButtons.forEach(elem => {
  elem.addEventListener("click", event => toggleModal(event.currentTarget.closest(".modal").id));
});
modals.forEach(elem => {
  elem.addEventListener("click", event => {
    if(event.currentTarget === event.target) toggleModal(event.currentTarget.id);
  });
});

// Close Modal with "Esc"...
document.addEventListener("keydown", event => {
  if(event.keyCode === 27 && document.querySelector(".modal.modal-show")) {
    toggleModal(document.querySelector(".modal.modal-show").id);
  }
});

function toggleModal(modalId) {
  const modal = document.getElementById(modalId);

  if(getComputedStyle(modal).display==="flex") { // alternatively: if(modal.classList.contains("modal-show"))
    modal.classList.add("modal-hide");
    setTimeout(() => {
      document.body.style.overflow = "initial";
      modal.classList.remove("modal-show", "modal-hide");
      modal.style.display = "none";      
    }, 200);
  }
  else {
    document.body.style.overflow = "hidden";
    modal.style.display = "flex";
    modal.classList.add("modal-show");
  }
}
</script>
<script>
  $(document).ready(function() {
$('.menu-toggle').click(function(){
  $('nav').toggleClass('active');
  $('body').toggleClass('fixback');
  $('#sym ul li').toggleClass('fdis');
})

$('ul li').click(function(){
  $(this).siblings().removeClass('active');
  $(this).toggleClass('active');
})
})
</script>


<script>


  window.onscroll = () => {
    const nav = document.querySelector('header');
    const logo= document.querySelector('header a img');
  
    if(this.scrollY <= 10){
      nav.className = 'mainhead'; 
      logo.className='navlogo'
    } 
    else
    {
      nav.className = 'navbarscrolled';
      logo.className='scrolledlogo'
    }
    
  };
  </script>
</body>
</html>